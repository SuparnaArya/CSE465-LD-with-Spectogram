{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import dependencies "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, RepeatVector, Reshape, Concatenate, UpSampling2D\nimport numpy as np\nimport librosa\nimport os\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"adding dataset and directory markup for test, train"},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = [\"de\", \"en\", \"es\"]\nlabel = np.array([1, 0, 0])\nfiles = []\ny = []\nfor cls in classes:\n    path = '../input/spoken-languages/train/' + cls + '/'\n    names = os.listdir(path)\n    for pos in range(len(names)):\n        files.append(path + names[pos])\n        y.append(label)\n    label = np.roll(label, 1)\n\nfiles = np.array(files)\ny = np.array(y)\npaths_train, paths_valid, y_train, y_valid = train_test_split(files, y, test_size = 0.2, random_state=45)\nprint(paths_train.shape, y_train.shape, paths_valid.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batch_generator(for_train, batch_size):\n    while True:\n        if for_train is True:\n            idx = np.random.randint(0, paths_train.shape[0], batch_size)\n        else:\n            idx = np.random.randint(0, paths_valid.shape[0], batch_size)\n            \n        x = np.zeros((batch_size, 39, 1001, 1))\n        y = np.zeros((batch_size, 3))\n        for i in range(batch_size):\n            if for_train is True:\n                audio, sr = librosa.load(paths_train[idx[i]], sr=16000)\n                y[i] = y_train[idx[i]]\n            else:\n                audio, sr = librosa.load(paths_valid[idx[i]], sr=16000)\n                y[i] = y_valid[idx[i]]\n            #extracting features for audio data through melspectogram    \n            mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=40, hop_length=int(0.010*sr), n_fft=int(0.025*sr))\n            if mfcc.shape[1] < 1001:\n                mfcc = np.concatenate((mfcc, np.zeros((mfcc.shape[0], 1001-mfcc.shape[1]))), axis=1)\n            else:\n                mfcc = mfcc[:, 0:1001]\n            x[i, :, :, 0] = mfcc[1:]\n            \n        yield x, y","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}